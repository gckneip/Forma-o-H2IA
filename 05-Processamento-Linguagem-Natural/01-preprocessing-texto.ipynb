{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento de Texto\n",
    "\n",
    "Neste notebook, aprenderemos técnicas essenciais para preparar dados de texto para modelos de NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapas do Pré-processamento de Texto\n",
    "\n",
    "1. **Limpeza**: Remover caracteres especiais, HTML, etc.\n",
    "2. **Tokenização**: Dividir texto em palavras/tokens\n",
    "3. **Normalização**: Converter para minúsculas\n",
    "4. **Remoção de Stopwords**: Remover palavras comuns\n",
    "5. **Stemming/Lemmatização**: Reduzir palavras à raiz\n",
    "6. **Vetorização**: Converter texto em números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Limpeza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    \"\"\"Remove caracteres especiais e normaliza o texto.\"\"\"\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Remover URLs\n",
    "    texto = re.sub(r'http\\S+|www\\S+', '', texto)\n",
    "    \n",
    "    # Remover menções e hashtags\n",
    "    texto = re.sub(r'@\\w+|#\\w+', '', texto)\n",
    "    \n",
    "    # Remover pontuação\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remover números\n",
    "    texto = re.sub(r'\\d+', '', texto)\n",
    "    \n",
    "    # Remover espaços extras\n",
    "    texto = ' '.join(texto.split())\n",
    "    \n",
    "    return texto\n",
    "\n",
    "# Exemplo\n",
    "texto_original = \"Olá! Visite nosso site: https://exemplo.com #IA @usuario123 :-)\"\n",
    "texto_limpo = limpar_texto(texto_original)\n",
    "\n",
    "print(\"Original:\", texto_original)\n",
    "print(\"Limpo:\", texto_limpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar(texto):\n",
    "    \"\"\"Divide o texto em tokens (palavras).\"\"\"\n",
    "    return texto.split()\n",
    "\n",
    "texto = \"processamento de linguagem natural é fascinante\"\n",
    "tokens = tokenizar(texto)\n",
    "\n",
    "print(\"Texto:\", texto)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Número de tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remoção de Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords comuns em português\n",
    "STOPWORDS_PT = set([\n",
    "    'o', 'a', 'os', 'as', 'um', 'uma', 'de', 'do', 'da', 'dos', 'das',\n",
    "    'em', 'no', 'na', 'nos', 'nas', 'por', 'para', 'com', 'sem',\n",
    "    'e', 'ou', 'mas', 'que', 'qual', 'quando', 'onde', 'como',\n",
    "    'é', 'são', 'foi', 'ser', 'estar', 'ter', 'fazer',\n",
    "    'este', 'esse', 'aquele', 'isso', 'isto', 'aquilo'\n",
    "])\n",
    "\n",
    "def remover_stopwords(tokens, stopwords=STOPWORDS_PT):\n",
    "    \"\"\"Remove stopwords da lista de tokens.\"\"\"\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "# Exemplo\n",
    "texto = \"o processamento de linguagem natural é uma área importante da inteligência artificial\"\n",
    "tokens = tokenizar(limpar_texto(texto))\n",
    "tokens_sem_stop = remover_stopwords(tokens)\n",
    "\n",
    "print(\"Tokens originais:\", tokens)\n",
    "print(\"Tokens sem stopwords:\", tokens_sem_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_bow(documentos):\n",
    "    \"\"\"Cria uma representação Bag of Words.\"\"\"\n",
    "    # Criar vocabulário\n",
    "    vocabulario = set()\n",
    "    for doc in documentos:\n",
    "        vocabulario.update(tokenizar(limpar_texto(doc)))\n",
    "    \n",
    "    vocabulario = sorted(list(vocabulario))\n",
    "    \n",
    "    # Criar vetores\n",
    "    vetores = []\n",
    "    for doc in documentos:\n",
    "        tokens = tokenizar(limpar_texto(doc))\n",
    "        vetor = [tokens.count(palavra) for palavra in vocabulario]\n",
    "        vetores.append(vetor)\n",
    "    \n",
    "    return vocabulario, np.array(vetores)\n",
    "\n",
    "# Exemplo\n",
    "documentos = [\n",
    "    \"Eu gosto de inteligência artificial\",\n",
    "    \"Machine learning é parte da inteligência artificial\",\n",
    "    \"Deep learning é fascinante\"\n",
    "]\n",
    "\n",
    "vocab, bow_matrix = criar_bow(documentos)\n",
    "\n",
    "print(\"Vocabulário:\", vocab)\n",
    "print(\"\\nMatriz BoW:\")\n",
    "print(bow_matrix)\n",
    "\n",
    "# Visualizar como DataFrame\n",
    "df_bow = pd.DataFrame(bow_matrix, columns=vocab)\n",
    "print(\"\\nBoW DataFrame:\")\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Criando o vetorizador TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Transformando os documentos\n",
    "tfidf_matrix = tfidf.fit_transform(documentos)\n",
    "\n",
    "# Visualizando\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análise de Frequência de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analisar_frequencia(texto, top_n=10):\n",
    "    \"\"\"Analisa as palavras mais frequentes.\"\"\"\n",
    "    tokens = tokenizar(limpar_texto(texto))\n",
    "    tokens = remover_stopwords(tokens)\n",
    "    \n",
    "    frequencias = Counter(tokens)\n",
    "    mais_comuns = frequencias.most_common(top_n)\n",
    "    \n",
    "    # Plotar\n",
    "    palavras = [palavra for palavra, freq in mais_comuns]\n",
    "    contagens = [freq for palavra, freq in mais_comuns]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(palavras, contagens)\n",
    "    plt.xlabel('Palavras')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.title(f'Top {top_n} Palavras Mais Frequentes')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mais_comuns\n",
    "\n",
    "# Exemplo com texto maior\n",
    "texto_exemplo = \"\"\"\n",
    "Inteligência artificial está transformando o mundo. \n",
    "Machine learning e deep learning são áreas importantes da inteligência artificial.\n",
    "Processamento de linguagem natural permite que computadores entendam texto.\n",
    "Redes neurais profundas são a base do deep learning moderno.\n",
    "\"\"\"\n",
    "\n",
    "palavras_comuns = analisar_frequencia(texto_exemplo, top_n=8)\n",
    "print(\"\\nPalavras mais frequentes:\", palavras_comuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "1. Implemente uma função para realizar stemming em português\n",
    "2. Crie um pipeline completo de pré-processamento que combine todas as etapas\n",
    "3. Compare a representação BoW com TF-IDF em um conjunto de textos\n",
    "4. Analise a frequência de palavras em um artigo ou notícia real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código aqui\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
